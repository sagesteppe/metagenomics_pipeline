---
title: "Angiosperms 353 metagenomic pipeline"
author: "fuck you"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F)
knitr::opts_chunk$set(dpi = 300) 
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(fig.align="left") 
```

This document is gonna make your eyes burn. And not in a *Heaven's Gonna Burn Your Eyes* kinda way. Their will be heinous crimes committed against multiple shell script languages, python, and R - heck I'll probably throw in a couple against tex. 

Essentially *woe is me*, my masters was a tragicomedy of stupidity, and here I try to converge the most import details as scattered across HackMD scripts. The things that confuse you in here confuse me too bud, grab an old style a few bottles of Malort or Gin, and let's please hope you still smoke and live in a progressive enough area you can do so indoors - and grab a fresh pouch of Stokkebye. 

I am going to assume that you use Linux. If you are windows, well it's time to switch it over. If you are on Mac, hopefully you bought it hot. 

If you are like me, and you didn't want to go head's in on a linux distro here is some advice to get a dual boot going on Windows 10.

## 1: Installing Ubuntu on a dual boot advice

> ‘Free your mind and your software will follow’ - George Clinton mostly

I was able to largely follow two guides to accomplish this.

My instructions below are brief.

For the dualboot installation download the most recent stable version of Ubuntu as an ISO image. I have previously used Balena Etcher to flash the image onto a USB thumbdrive, but have switched to RUFUS for the last couple boots. I recommend RUFUS now.

Following this use an administrator account to access the ‘Command Line’ and ‘Run as Administrator’.

Now enter 'diskmgmt.msc', in the gui click on the ‘C:’ drive and ‘Shrink Volume’ of it. Shink to between 20,000-80,000mib. I did 75,000 - should be way overkill.

To dual boot I had to go with the old approach of the F12 at startup. If I remember this was the same route I had to use for Mint on a Lenovo ThinkPad T-530. So it could be a Lenovo thing.
1a: Troubleshooting Windows Shutdown

As a consequence of dual booting it may be hard to shutdown your hardware from the WindowsOS. This is a workaround, which will make it take longer to log into Windows, but will let your hardware shutdown.

This Unix & Linux StackExchange Thread had a solution which worked for me.
https://unix.stackexchange.com/questions/247184/unable-to-shutdown-windows-after-installing-grub

    (1) Go to control panel.
    (2) Find power options.
    (3) From the left menu click on “Choose what the power button does”.
    (4) Click on “Change settings that are currently unavailable.”
    (5) Go to the bottom of the page, uncheck “Turn on fast startup” and save changes.

After doing the typical updates and processes associated with installing Linux you are good to go!
1b: Troubleshooting Time Zones

You’ll find on your Windows OS the time is probably wrong. Run this on the linux side of the computer to fix this. PS. to access the command line, **CONTROL + ALT + T**.  

```{sh, eval = F}
timedatectl set-local-rtc 1
```

Now you need to update the time manually in windows and you should be good.

## 2: Check to see if the data transfer from the genomics core was clean

Strange things happen here and we do not know why.

Here we have some quick lines to inspect the files you transferred over in order to figure out if you are missing something.
```{sh}
$ mkdir /media/sagesteppe/Genomics/data_summaries

# A) find distinct Sample ID's.
$ cd /media/sagesteppe/Genomics/Original_zip
$ find . -regex ".*\_S[0-9][0-9]?[0-9]?_.*" > ../data_summaries/sample_ids.txt
$ sed -i -E "s/.*(S[0-9][0-9]?[0-9]?).*/\1/" ../data_summaries/sample_ids.txt
$ sort -u ../data_summaries/sample_ids.txt > ../data_summaries/unique_seqs.txt

# B) count the distinct samples to determine how many are present.
$ wc -l ../data_summaries/unique_seqs.txt

# C) determine which samples have each of 4 reads associated with them. 
$ sort ../data_summaries/sample_ids.txt | uniq -c > ../data_summaries/reads_per_sample.txt

# D) Identify the missing & extra reads.
$ awk ' $1 < 7 { print }' ../data_summaries/reads_per_sample.txt > ../data_summaries/missing_reads.txt
$ awk ' $1 > 8 { print }' ../data_summaries/reads_per_sample.txt > ../data_summaries/extra_reads.txt

# E) Report the missing & extra reads.
$ cat ../data_summaries/missing_reads.txt
$ cat ../data_summaries/extra_reads.txt

# F) Determine which reads are empty. 
" find ~/lists -empty " # not solved. 

# G) Determine if any samples are missing. 
$ sed -i -E 's/S//g' ../data_summaries/unique_seqs.txt # remove 'S'
$ sort -n ../data_summaries/unique_seqs.txt -o ../data_summaries/unique_seqs.txt # arrange numerically

$ min=$(sort -n ../data_summaries/unique_seqs.txt | sed -n '1p') #lowest sample number
$ max=$(sort -n ../data_summaries/unique_seqs.txt | sed -n '$p') # highest sample number
$ seq "$min" 1 "$max" > ../data_summaries/full_seq.txt # create range

$ difference=$(( $(wc -l ../data_summaries/full_seq.txt |  sed 's/[^0-9]*//g') - $(wc -l ../data_summaries/unique_seqs.txt | sed 's/[^0-9]*//g') )) # see how many samples in results
$ echo "$difference"  # compared to range of all sample numbers

$ comm -3 ../data_summaries/unique_seqs.txt ../data_summaries/full_seq.txt
# you may get a message in your output like "comm: file 2 is not in sorted order" - but for me it is so... if you do receive any output from this you are good.

# H) Determine file size of read. (In MiB)
$ du -a --block=1M > ../data_summaries/fsize_raw_reads.txt

$ cd
```



## 3: Install somestuff

### Download Anaconda

You can go to the [Anaconda downloads](https://www.anaconda.com/products/distribution?gclid=Cj0KCQjw_4-SBhCgARIsAAlegrXAjXe0A4iqM2IqbZZkJigwqBOai13UOABusXpOAwnzPrNec4mJgdcaAtuCEALw_wcB) page and find the version relevant to you and copy the link. It is easier to install from command line then clicking and dragging IMO. 
```{sh}
sudo apt-get update -y
sudo apt-get upgrade -y

cd /tmp
wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh
bash Anaconda3-2021.11-Linux-x86_64.sh
```

Now you will need to respond to some prompts in the terminal. I let my conda install directly within my user profile. You should now have conda installed.

so your terminal looked like this: `sh reed@reed-steppe:~` BUT NOW it looks like this:  `sh (base) reed@reed-steppe:~`, maybe you didn' notice a change here if you are new to this, but if you got the based ya got what ya want. 

Check the version of Conda, and check for updates and install them. 
```{sh}
conda --version
conda update conda
```

### CONDA ON

From here on out we are going to work in the Conda environment. 

We will now create an environment to store all of our metagenomics materials in. These environments serve to isolate the contents of projects.From now on pretty much everything we are going to do should be occuring in this 'environment', to illustrate the point

```{sh}
conda create --name metagenomics
conda activate metagenomics # we always need to activate this !!!!
conda info --envs
```

Now we will turn to installing biopython - make sure you are in the right project ('metagenomics' !)

We will now install biopython, which is prone to fits of rage for most people, but generally plays nicely with conda `sh conda install biopython`. 

```sh $PATH```
bash: /home/reed/anaconda3/envs/metagenomics/bin:/home/reed/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin: No such file or directory

Notice that each of the above paths are *within* the anaconda3/envs. 

### Github

All linux are meant to ship with GH, I think even if you do the super spartan install. Although maybe they just ship with Git since gh changed hands. Anyways. 
```{sh}
$ git --version
# if you do not have it...
$ sudo apt install git-core
```

### Homebrew 

this is supposedly way easier on Mac's, however we should not condone sweat shop labor. 

```{sh}
# ensure you are still in metagenomics folder if not conda activate metagenomics !!!!
sudo apt install curl
git clone https://github.com/mossmatters/HybPiper.git

/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"
echo 'eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"' >> /home/reed/.profile
eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
sudo apt update -y
brew install blast bwa gcc parallel spades samtools
parallel --bibtex

sudo apt-get update -y
sudo apt-get upgrade -y

sudo apt-get install -y exonerate # now let's grab exonerate!!!
```

Ensure the file is in a suitable location such as...
```
cd /home/reed/HybPiper
python3 reads_first.py --check-depend
```

this should hopefully say all packages can be found!

```
cd /test_dataset
bash run_tests.sh
```
if you run this and all kinds of stuff happens that is good.


### Create a big bad bioinformatics folder

```sh
mkdir sequenceData
cd sequenceData
```
copy the original files over to your computer
```sh
cp -R /media/reed/Genomics/Original_zip . 
```

### FastQC

this worked for me 

```{sh, eval = F}
sudo apt update -y
sudo apt upgrade -y
sudo apt install fastqc # install

$ mkdir /media/sagesteppe/Genomics/Pre_trim_FastQC # to hold output
$ cd /media/sagesteppe/Genomics/Original_zip # fastqs to evaluate
$ fastqc *fastq.gz --outdir=/media/sagesteppe/Genomics/Pre_trim_FastQC -t 3 # torun process

$ threads=$(nproc --all)
$ threads=$(($threads - 1))
$ echo $threads # to print results. 

# hopefully dynamic using all threads - 1
$ fastqc *fastq.gz --outdir=/media/sagesteppe/Genomics/Pre_trim_FastQC -t echo$threads

```

### Trimmomatic

Most of my trimmomatic advice came from a blog hosted by the [University of Texas at Austin](https://wikis.utexas.edu/display/bioiteam/Trimmomatic+-+GVA2020), and John Zhang.

This is what worked for me, I did a few things they did not. Realistically all of this advice should be dumb and you should be able to install it layers beneath this.

```{sh}

cd /usr/bin
wget http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.39.zip

unzip Trimmomatic-0.39.zip
rm Trimmomatic-0.39.zip
export PATH=$PATH:/home/reed/Trimmomatic-0.39/

java -jar Trimmomatic-0.39/trimmomatic-0.39.jar
```

OK so trimmomatic is installed, but it is a long path to call it, so we will create a bash script to do that. One way to do that is this.
```{sh}
nano Trimmomatic-0.39/trimmomatic #You will now have a strange new terminal before you. Don't Panic. Place this, or a similar command into **nano** (*the weird new terminal*). 
```

**--NANO INTERFACE START--**

```{sh}
!/bin/bash
java -jar $HOME/Bioinformatics/Trimmomatic-0.39/trimmomatic-0.39.jar $@`
```
Note that the #!/bin/bash argument has to be on it's own line. It is defining that the .txt file you are writing is meant to be processed by Bash.

Now to safely exit nano, on your keyboard: 
> **ctrl + X**  

and to to close the terminal click **'Y'** to say 'YES'

Now this is an important part, it may ask if you want to update the name of the script you just wrote - do not! My script name is never displayed, but you have already supplied the name and the directory for it to be saved too!

On your keyboard hit **ENTER**

**--NANO INTERFACE END--**

Now we will need to make it so that this script is executable. We can do this as so: `{sh} $ chmod +x ~/Bioinformatics/Trimmomatic-0.39/trimmomatic`

And now you should be able to run Trimmomatic by:
`sh trimmomatic`

Note if you closed your terminal you will need to re-add the folder to the $PATH. You can add this to the $PATH forever, 

### Install Mega353 for custom target files

```{sh}
python3.9 -m pip install pandas # dependency
cd ~/.local/bin
~/.local/bin$ git clone https://github.com/chrisjackson-pellicle/NewTargets.git
$ cd ~/.local/bin/NewTargets
~/.local/bin/NewTargets$ unzip mega353.fasta.zip
# cd is not the best way to do the  last step but how i did it.
```


### Install R and Rstudio

these steps worked to install R
```{sh}
sudo apt update -qq
sudo apt install --no-install-recommends software-properties-common dirmngr
sudo wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
sudo add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/"
apt install --no-install-recommends r-base 
sudo usermod -a -G staff sagesteppe
sudo apt-get update -y
sudo apt-get install -y libssl-dev
sudo apt-get install libcurl4-openssl-dev
sudo add-apt-repository ppa:c2d4u.team/c2d4u4.0+
sudo apt install --no-install-recommends r-cran-rstan
sudo apt-get install libtiff-dev libjpeg-dev libpng-dev
sudo apt-get install libblas-dev liblapack-dev
```

to install rstudio the following sometimes works
```{sh}
sudo add-apt-repository universe
sudo apt-get install gdebi-core
sudo apt install dpkg-sig
cd /home/sagesteppe/Downloads
    
~/Downloads$ gpg --keyserver keyserver.ubuntu.com --recv-keys 3F32EE77E331692F

wget --no-check-certificate https://www.rstudio.com#/products/rstudio/download/#download
curl -k -O -L https://www.rstudio.com/products/rstudio/download/#download" "rstudio-2021.09.1-372-amd64.deb"
    
~/Downloads$ dpkg-sig --verify rstudio-2021.09.1-372-amd64.deb
~/Downloads$ ls *.deb
~/Downloads$ sudo gdebi ./rstudio-2021.09.1-372-amd64.deb
~/Downloads$ rstudio # ensure installation works. 
> quit() # (in gui)
~/Downloads$ rm rstudio-2021.09.1-372-amd64.deb
~/Downloads$ cd 
```


## Process read data


### Run trimmomatic on all data

```{sh}
conda activate metagenomics #(if not already)

cd sequenceData
mkdir raw_reads
find Original_zip/ -type f -print0 | xargs -0 mv -t raw_reads/ # copy all of our reads out of the original
mkdir Trimmed_reads
cp ~/Trimmomatic-0.39/adapters/TruSeq3-PE.fa raw_reads

trimmomatic PE -phred33  1a_S76_L002_R1_001_paired.fastq.gz 1a_S76_L002_R2_001_paired.fastq.gz -baseout ../Trimmed_reads/1a_S76.fastq.gz ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 SLIDINGWINDOW:5:20 MINLEN:36

```

### Prepare files for Mega353

We want to use HybPiper on our reference taxa, i.e. the plant species which we hope to identify in our environmental samples which we could not find existing sequence data for online and sequenced ourselves. We will not put our environmental samples through this pipeline.

We will need to:

    Identify which reads these are.
    Gather some systematic information on them.
    Create a Mega353 ‘select_file.txt’

Take a look at what taxa we have reads from real quick, and look at what is required to use M353. 
```{sh}

cd ~/.local/bin/NewTargets
column -s, -t < filtering_options.csv | less -#2 -N -S 
cd
cp filtering_options.csv /media/sagesteppe/Genomics/filtering_options.csv
    
cp .local/bin/NewTargets/select_file_example.txt /media/sagesteppe/Genomics/select_file_example_cp.txt # remove later
```

type any alphabetical letter like **‘q**’ to quit the view mode. Regarding M353,  for me the easiest option is to try and join the taxonomic information regarding my reference sample to either the Order or family in this file.

My notes on my extractions are in an excel spreadsheet. While we can use this in Python, lets convert to a CSV.
```{sh}
cd /media/sagesteppe/Genomics
ls *.xlsx # to check file name
soffice --headless --convert-to csv Prospective_samples.xlsx
rm Prospective_samples.xlsx
```

in order to gather the taxonomic information for the reference samples we can do something as so

```{r}
# $ R # this from the command line - it will engage the R language for statistical computing
install.packages("taxize") # note 'Y', 'Y' for the two options.
library(taxize)
setwd("/media/sagesteppe/Genomics")
f = read.csv("Prospective_samples.csv")
# > head(f)
f = f[c('Voucher_specimen','Number','Sample', 'type', 'vials')]
f = subset(f, type == 'library')
f = f[c('Number','Sample', 'vials')]
f$Sample = gsub("_", " ", f$Sample)
# note NCBI subsumes L. bakeri into synonymy with L. sericeus
f[f == 'Lupinus bakeri'] = 'Lupinus sericeus'
f[f == 'Erigeron elatior'] = 'Erigeron grandiflorus' 
# similar to above
taxa = f$Sample
ENTREZ_KEY = '1dddef4ac326bf8b74fa69ae430411218f09'
results = classification(taxa, db = 'ncbi')
    
family = lapply(results, subset, rank == "family")
family = do.call(rbind, family)
family = cbind(rownames(family), data.frame(family, row.names=NULL))


colnames(family) = c('Sample', 'Family')

# repeat for order. 

order = lapply(results, subset, rank == "order")
order = do.call(rbind, order)
order = cbind(rownames(order), data.frame(order, row.names=NULL))
order = order[,1:2]
colnames(order) = c('Sample', 'Order')
        # combine
        
taxonomy = merge(family, order, on = "Sample")
f = merge(f, taxonomy, on = "Sample")
f[f == 'Lupinus sericeus'] = 'Lupinus bakeri'
f[f == 'Erigeron grandiflorus'] = 'Erigeron elatior'
    
rm(taxonomy, order, family, taxa)
write.csv(f, "Taxonomy_ref_lib_samples.csv", row.names = FALSE)
quit()
```

We can write the files to filter the M353 targets as so
```{py}
$ python3
# you are now in python 
import os
import pandas as pd
import numpy

taxonomy_ref = pd.read_csv(r'/media/sagesteppe/Genomics/Taxonomy_ref_lib_samples.csv')
targets = pd.read_csv(r'/media/sagesteppe/Genomics/filtering_options.csv')

df = taxonomy_ref.merge(targets, on= 'Order', how= 'left').drop(columns=['Family_y']).rename(columns={"Family_x": "Family"})
    
df = df.drop_duplicates(subset=['Target_name'])
orders = df['Order'].values.tolist()
orders = list(sorted(set(orders)))
    
df['Filepath'] = '/media/sagesteppe/Genomics/to_pipe/'
df["Filepath"] = df["Filepath"] + df["Order"] + '/'
df['Filename'] = df["Order"] + '_filter'
    
df = df[["Family", "Order", "Target_name", "Group", "Species", "Filepath", 'Filename']]

def m353_select(df):
    """ Write a text file for use with M353 that include information
  on phylogenetically appropriate samples for read mapping. Useful for
  mapping reads from plant species across multiple orders such
  as in meta-barcoding projects.

  Parameters
  ---------
  A single dataframe of all 1kp transcriptomes in M353, with
  your sample information, and systematic placements (Order- Species) of taxa.
  
  order /Order: Taxonomic rank, sensu APG4.
  fam /Family: Taxonomy rank, sensu APG4.
  sp /Species: taxon with sequences to map, generally from NCBI.
  trgt_n /Target_name: M353 target taxa using their codes.
  fname /File_path: filename of fastq to map.
  fpath/ Directory: directory from root to folder containing fastqs.
  
  A list of each fastq file which needs to go through HybPiper
  
  Returns
  --------
  A text file like the M353 example.
  """
    
    orders = df['Order'].values.tolist()
    orders = list(sorted(set(orders)))
    
    for x in orders:
    
      sub = str(x)
      df_sub = df[df.Order.eq(sub)]
    
      order = df_sub['Order'].values.tolist()[0]
      fam = df_sub['Family'].values.tolist()[0]
      trgt_n = df_sub['Target_name'].values.tolist()
      sp = df_sub['Species'].values.tolist()
      Filepath = df_sub['Filepath'].values.tolist()[0]
      fname = df_sub['Filename'].values.tolist()[0]
      
      fname: str = Filepath + fname
      filename = "%s.txt" % fname
    
      f = open(filename, "x")
      f.write("[Target_name]\n")
    
      with open(filename, 'a') as f:
        for item in trgt_n:
          f.write("%s\n" % item)
        
      f = open(filename, "a")
      f.write("\n[Group]\n \n[Order]\n%s \n\n[Family]\n%s \n\n[Species]\n%s" % (order, fam, sp))
      f.close()
    
m353_select(df) 
quit()
```

We will now move each set of sequence data to a folder associated with their Order so that we do not have to make many copies of M353 fastq. This should have been done sooner, but hopefully allows people to have more autonomy to fit there needs.

```{py}
$ python3
import os
import pandas as pd
import shutil
taxonomy_ref = pd.read_csv(r'/media/sagesteppe/Genomics/Taxonomy_ref_lib_samples.csv')
    
files = os.listdir('/media/sagesteppe/Genomics/to_trim/Trim_Reads/')
files = pd.DataFrame(files, columns = ['File_path'])
files["Files"] = files["File_path"]
files['Files'] = (files['Files'].replace('.fastq', '', regex=True).replace('_[0-9][P|U]','', regex=True).replace('_[R][1_2]','', regex=True).replace('_unpaired','', regex=True).replace('_paired','', regex=True).replace('_S[0-9]+', '', regex=True).replace('_', '', regex=True).replace('-', '', regex=True))
mask = ~files['File_path'].str.contains('_fastqc', case=False, na=False) # find fastqc
files = files[mask] # remove fastqc
files =  files[files['Files'].str.contains('[A-Za-z]')] # remove non reference library.
files['Files'] = files['Files'].str.lower()
files = files.merge(taxonomy_ref, left_on = 'Files', right_on = 'vials') 

parent_dir = "/media/sagesteppe/Genomics/to_pipe"
os.mkdir(parent_dir)
directory = files['Order'].values.tolist()
directory = list(set(directory))
paths = os.path.join(parent_dir, directory) # list of paths duhr
    
for i in directory:
    npath = os.path.join(parent_dir, i)
    os.mkdir(npath)   

c_path = '/media/sagesteppe/Genomics/to_trim/Trim_Reads/'
ord = files['Order'].values.tolist()
cu_paths = files['File_path'].values.tolist()
current_paths = []
for i in cu_paths:
    current_paths.append(os.path.join(c_path, i))
    
new_paths = []
new_paths = [parent_dir + '/' + a + '/' + b for a, b in zip(ord, cu_paths)]
    
list(map(shutil.move, current_paths, new_paths)) # pretty sure this is the one.
    
quit()
```


We can copy the M353 file to each directory as so:

```{py}
$ python3
import os
import shutil
m353_path = ("/home/sagesteppe/.local/bin/NewTargets/mega353.fasta.zip")  
dest_path = '/media/sagesteppe/Genomics/to_pipe'
os.chdir(dest_path)
directories = next(os.walk('.'))[1]
new_paths = []
new_paths = [dest_path + '/' + a + '/m353.fasta.zip'for a in directories]

for path in new_paths:
  shutil.copy(m353_path, path)
quit() # all copied
```

```{sh}
cd '/media/sagesteppe/Genomics/to_pipe' 
find . -name '*.zip' -exec sh -c 'unzip -d `dirname {}` {}' ';' # extract .zip
find . -type f -name '*.zip' -delete # delete .zip

# and we can filter it as so… Note I could not make this work programmatically (and did not put in much effort)

cd '/media/sagesteppe/Genomics/to_pipe/Rosales'
python3 /home/sagesteppe/.local/bin/NewTargets/filter_mega353.py mega353.fasta Rosales_filter.txt
    
# if you are done you can remove the original m353.fasta
    
cd '/media/sagesteppe/Genomics/to_pipe'
find . -maxdepth 2 -type f -iname 'mega353.fasta' -exec rm -rf {} \;
```


### Run Hyb-Piper

We will unfortunately need to change to each Orders directory to do this

```{sh}
# we can then create this script in the root of the folder with all of our sequences by order

for file in *R1_paired*; do
file2=${file//R1/R2}; file3=${file//R1_paired.fastq/unpaired.fastq}; file4=${file//R1_paired.fastq/}
echo python /home/reed/HybPiper/reads_first.py -r $file $file2 --unpaired $file3 -b filtered_target_file.fasta --prefix $file4 --bwa >> reads_first.sh; done

# and copy it to each folder as so:

for d in */; do cp reads_first.sh "$d"; done
rm reads_first.sh # to remove the script from the root

# and then run it from each folder
bash reads_first.sh
```


## Build the Kraken database

A very thorough review of how to install Kraken is provided below.
https://github.com/DerrickWood/kraken2/wiki/Manual

Steps for prepping a novel Kraken database: https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown#custom-databases 

Let us download the taxonomy which NCBI uses, We have it laying around for R, but I do not want to troubleshoot linking it to kraken myself. It is also not a very large file. We will also download the complete set of Plant Genomes and proteins.

```{sh}
kraken2-build --download-taxonomy --db $pl_mtg
kraken2-build --download-library plant --db $pl_mtg
```

### prep novel sequence data

We will now add our novel sequence data to the database. First we need to find their NCBI taxid.

```{r}
$ r # launch r from the terminal
install.packages('taxize')

species = read.csv ('/media/sagesteppe/Genomics/Taxonomy_ref_lib_samples.csv')
species_list = species_list[,1]
species_list = unique(species)
species_ncbi = taxize::classification(species_list, db = 'ncbi')
results = lapply(species_ncbi, function(x) x[nrow(x),c(3)])
results = t(rbind(unlist(results)))
results = cbind(rownames(results), data.frame(results, row.names=NULL))
names(results) = c('Sample','NCBI_code')

species = merge(species, results, by = 'Sample' )
# write.csv ('/media/sagesteppe/Genomics/Taxonomy_ref_lib_samples.csv', append = FALSE)
rm(species_list, species, results)
quit()
```

Now we append the taxid to the top line of each file before the first whitespace.

sequence16|kraken:taxid|32630 Adapter sequence # example

```{sh}
cd ~/sequenceData/to_pipe/Rosales_cp

for i in ./*/*/*contigs.fasta; do
   n=$(basename -s _contigs.fasta "$i")
   n1=$(dirname "$i")
   n2="$(dirname $n1)"
   n3="$(basename $n2)"
   n4="$n|kraken:taxid|$n3"
   sed "s/^\(>NODE.*\)/\1-$n4/" "$i"
done > testy.fasta
```

Now look up and join the NCBI identifier after the kraken taxid code. 

```{sh}
> R
library(taxize)

data <- read.csv('/media/reed/Genomics/Taxonomy_ref_lib_samples.csv')
data$Sample <- gsub('Lupinus bakeri', 'Lupinus sericeus', data$Sample)
data$Sample <- gsub('Erigeron elatior','Erigeron grandiflorus', data$Sample)

species <- unique(data$Sample)
pc <- get_uid(sci_com = species, key = '1dddef4ac326bf8b74fa69ae430411218f09')
ncbi_keys <- cbind(species, pc)

taxid_results <- merge(data, ncbi_keys, by.x = "Sample", by.y =  "species",  all.x = TRUE)
rm(species, pc, ncbi_keys, data)

directory <- '/home/reed/sequenceData/to_pipe/'
list.dirs.depth.n <- function(p, n) { # @ RolandASC
  res <- list.dirs(p, recursive = FALSE)
  if (n > 1) {
    add <- list.dirs.depth.n(res, n-1)
    c(res, add)
  } else {
    res
  }
}

folder_name <- list.dirs.depth.n(p = directory, n = 2)
folder_name <- sub('^.*/', '', folder_name)
folder_name <- grep("_", folder_name, value = TRUE)
folder_name <- tolower(folder_name)
sample_folder_names <- folder_name
sample_folders <- sub('_s[0-9]{2}_', '', folder_name)
sample_folders_squished <- gsub('-|_', '', sample_folders)

targets <- data.frame(folder_path = paste0(directory, folder_name), 
                      sample_folders,
                      sample_folders_squished, 
                      sample_folder_names)

rm(sample_folders_squished, folder_name, sample_folder_names, sample_folders, directory, list.dirs.depth.n)

final_results <- merge(taxid_results, targets, 
                       by.x = 'vials', by.y = 'sample_folders_squished',
                       all.x = T)
final_results <- final_results[c('vials','pc', 'sample_folder_names')]
#write.csv(final_results, 'taxid_lookup_table.csv', row.names = F)
final_results_text <- final_results[,c('sample_folder_names', 'pc')]
final_results <- final_results[!duplicated(final_results),]
final_results_text$sample_folder_names <- gsub('s', 'S', final_results_text$sample_folder_names)
readr::write_delim(final_results_text, file = "taxid_lookup.txt")

rm(taxid_results, targets, final_results)
quit()
```

```{sh}
cd ~/sequenceData/to_pipe

awk '
NR==FNR {
    old[NR] = $1
    gsub(/&/,RS,$2)
    new[NR] = $2
    next
}
{
    for (i=1; i in old; i++) {
        gsub(old[i],new[i])
    }
    gsub(RS,"\\&")
    print
}
' taxid_lookup.txt ./*/testy.fasta

```

4a.2 supplement database with ToL data

OK now we will see what we have, and what else we want from Kew.

While the repository for the ToL sequence data is the European Nucleotide Archive, it is much easier to access these data via NCBI SRA. It seems easiest to manually navigate to the [SRA RUN Selector](https://www.ncbi.nlm.nih.gov/Traces/study/?query_key=10&WebEnv=MCID_61d4719c8f8e1f11220da754&o=acc_s%3Aa) and manually download the ‘Total’ 'Metadata ’ csv file.

We can open this file in python and compare it to the target geographically filtered material for our study area. We will also focus on flowering plants less Poales.

We will use a table which can look up clades from genera from “[SRA RUN Selector](https://www.ncbi.nlm.nih.gov/Traces/study/?query_key=10&WebEnv=MCID_61d4719c8f8e1f11220da754&o=acc_s%3Aa). I hoped to use taxizedb here, but it is unable to install on Ubuntu and as Scott left ROpenSci I think it has fell out of maintenance.

```{py}
$ python3
>>> import pandas as pd
sra_table = pd.read_csv("/media/sagesteppe/Genomics/data_summaries/SraRunTable.txt")
sra_table = sra_table.iloc[:, [0,1,2,12,22,33,34,35,36,37,38,39,40,41]]
sra_table = sra_table[~(sra_table.Organism == 'unidentified')]
sra_table = sra_table[~sra_table.Organism.str.contains(" sp\\.")]
sra_table[['Genus','Species']] = sra_table['Organism'].str.split(' ', 1, expand=True)

lookup = pd.read_csv("~/Bioinformatics/files/Spermatophyta_Genera.csv")
lookup = lookup.iloc[:,[0,1,2]] 
lookup = lookup.rename(columns={lookup.columns[0]: "Genus"})  
sra_table = sra_table.merge(lookup, on = 'Genus', how = 'left') 

site_target_taxa = pd.read_csv("/hdd/MS_SDM_RMBL/SDMS_RMBL/data/processed/taxa_predicted_rmbl.csv") 
site_target_taxa['name'].str.split('_', 1, expand=True)

    
    # alternate route
df2 = pd.DataFrame(site_target_taxa.name.str.split(pat = '_').tolist(), columns="Genus Epithet".split())
genera = df['Genus'].tolist()
    
    #original route
genera = site_target_taxa['Genus'].tolist()
genus_filter = sra_table['Genus'].isin(genera)
sra_sub = sra_table[genus_filter]
print(sra_sub.to_string())
sra_requests = sra_sub.iloc[:,0] 
sra_requests.to_csv("~/sequenceData/files/sra_requests_taxonomy.csv", header=False, index = False) 
    
sra_requests_to_scaffold = sra_sub.iloc[:,[0,3,14,15,16,17]]
sra_requests_to_scaffold.to_csv("/media/reed/Genomics/kew_tol/Trim_Reads/sra_to_scaffold.csv", header=False, index = False) 
quit()
```
We will now download a program to make downloading data from SRA easier.

[SRA toolkit](https://github.com/ncbi/sra-tools/wiki/02.-Installing-SRA-Toolkit) is installed via:
```{sh}
$ cd /home/sagesteppe/.local/bin
$ wget --output-document sratoolkit.tar.gz http://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-ubuntu64.tar.gz
$ tar -vxzf sratoolkit.tar.gz
$ rm sratoolkit.tar.gz
$ ls # note your file will have a different name.  but be sure to keep the ''/bin' 
$ export PATH=$PATH:$PWD/sratoolkit.2.11.2-ubuntu64/bin
    
$ cd /home/sagesteppe/.local/bin/sratoolkit.2.11.2-ubuntu64/bin
$ which ./fastq-dump
```
refer to [this page](https://github.com/ncbi/sra-tools/wiki/03.-Quick-Toolkit-Configuration) for configuration in a bash script

`sh $ ./fastq-dump --stdout SRR390728 | head -n 8 ` and compare it to the output on the main git page. only exception is you will have something after the message they use an example ‘fastq-dump --stdout SRR390728 | head -n 8’, which should pop up on linux.

Quick Toolkit configuration

Now you can start to configure the SRA toolkit refer to the link above for the steps.
`sh vdb-config -i`
Once you have the SRA toolkit installed you can do the following to ‘prefetch’ the reads. It took me about 1.5 hours to get 126 samples on a Friday using Northwesterns internet via ethernet running at 900 Mbps/s

```{sh}
./prefetch --option-file ~/sequenceData/sra_requests.csv

# now extract and move the prefetchs

for file in /home/reed/sra_prefetch/sra *.sra
do
    ~/.local/sratoolkit.3.0.0-ubuntu64/bin/fasterq-dump /home/reed/sra_prefetch/sra/*.sra  -t /tmp/scratch -e 16 -O /media/reed/Genomics/kew_tol -p
done

## for the second batch of pre fetchs we have already 
~/Genomics/kew_tol $ mkdir supplemental_sra
# and will now copy them to:

for file in /home/reed/sra_prefetch/sra *.sra
do
    ~/.local/sratoolkit.3.0.0-ubuntu64/bin/fasterq-dump /home/reed/sra_prefetch/sra/*.sra  -t /tmp/scratch -e 16 -O /media/reed/Genomics/kew_tol/supplemental_sra -p
done
```


Now we will have to trim these sequences. 

```{sh}
$ mkdir Trim_Reads
$ for r1 in *_1*.fastq; do r2=$(echo $r1|sed 's/_1/_2/'); name=$(echo $r1|sed 's/_1.fastq.gz//'|sed 's/Raw_Reads\///'); echo "java -jar ~/Trimmomatic-0.39/trimmomatic-0.39.jar PE $r1 $r2 -baseout Trim_Reads/$name.fastq.gz ILLUMINACLIP:/home/reed/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:5:20 MINLEN:30";done > trim_commands
$ bash trim_commands
```


and as before we will copy them into there own folders to run a m353 file with hyb piper.
```{py}
$ python3
import os
import pandas as pd
import shutil
taxonomy_ref = pd.read_csv(r'./Trim_Reads/sra_to_scaffold.csv', header = None)
taxonomy_ref.columns = ['sample','species','genus','epithet', 'family','order']

files = os.listdir('Trim_Reads') 
files = pd.DataFrame(files, columns = ['File_path'])
files[files["File_path"].str.contains("gz")]
files["Files"] = files["File_path"]
files['Files'] = (files['Files'].replace('_.*', '', regex=True))
files = files.merge(taxonomy_ref, left_on = 'Files', right_on = 'sample') 

parent_dir = "/media/reed/Genomics/kew_tol/to_pipe"
os.mkdir(parent_dir)
directory = files['order'].values.tolist()
directory = list(set(directory))
    
for i in directory:
    npath = os.path.join(parent_dir, i)
    os.mkdir(npath)   

c_path = '/media/reed/Genomics/kew_tol/Trim_Reads'
ord = files['order'].values.tolist()
cu_paths = files['File_path'].values.tolist()
current_paths = []
for i in cu_paths:
    current_paths.append(os.path.join(c_path, i))
    
new_paths = []
new_paths = [parent_dir + '/' + a + '/' + b for a, b in zip(ord, cu_paths)]
    
list(map(shutil.move, current_paths, new_paths)) # finally move the files to the order specific folder

quit()
```


We can write the files to filter the M353 targets as so
```{py}
$ python3
import os
import pandas as pd
import numpy

taxonomy_ref = pd.read_csv(r'/media/reed/Genomics/kew_tol/Trim_Reads/sra_to_scaffold.csv')
taxonomy_ref.columns = ['Sample','Species','Genus','Epithet', 'Family','Order']
targets = pd.read_csv(r'/media/reed/Genomics/filtering_options.csv')

df = taxonomy_ref.merge(targets, on= 'Order', how= 'left').drop(columns=['Family_y']).rename(columns={"Family_x": "Family"}).rename(columns={"Species_y": "Species"})
    
df = df.drop_duplicates(subset=['Target_name'])
df['Order'] = df['Order'].astype(str)
orders = df['Order'].values.tolist()
orders = list(sorted(set(orders)))
    
df['Filepath'] = '/media/reed/Genomics/kew_tol/to_pipe/'
df["Filepath"] = df["Filepath"] + df["Order"] + '/'
df['Filename'] = df["Order"] + '_filter'
    
df = df[["Family", "Order", "Target_name", "Group", "Species", "Filepath", 'Filename']]

def m353_select(df):
    """ Write a text file for use with M353 that include information
  on phylogenetically appropriate samples for read mapping. Useful for
  mapping reads from plant species across multiple orders such
  as in meta-barcoding projects.

  Parameters
  ---------
  A single dataframe of all 1kp transcriptomes in M353, with
  your sample information, and systematic placements (Order- Species) of taxa.
  
  order /Order: Taxonomic rank, sensu APG4.
  fam /Family: Taxonomy rank, sensu APG4.
  sp /Species: taxon with sequences to map, generally from NCBI.
  trgt_n /Target_name: M353 target taxa using their codes.
  fname /File_path: filename of fastq to map.
  fpath/ Directory: directory from root to folder containing fastqs.
  
  A list of each fastq file which needs to go through HybPiper
  
  Returns
  --------
  A text file like the M353 example.
  """
    
    orders = df['Order'].values.tolist()
    orders = list(sorted(set(orders)))
    
    for x in orders:
    
      sub = str(x)
      df_sub = df[df.Order.eq(sub)]
    
      order = df_sub['Order'].values.tolist()[0]
      fam = df_sub['Family'].values.tolist()[0]
      trgt_n = df_sub['Target_name'].values.tolist()
      sp = df_sub['Species'].values.tolist()
      Filepath = df_sub['Filepath'].values.tolist()[0]
      fname = df_sub['Filename'].values.tolist()[0]
      
      fname: str = Filepath + fname
      filename = "%s.txt" % fname
    
      f = open(filename, "x")
      f.write("[Target_name]\n")
    
      with open(filename, 'a') as f:
        for item in trgt_n:
          f.write("%s\n" % item)
        
      f = open(filename, "a")
      f.write("\n[Group]\n \n[Order]\n%s \n\n[Family]\n%s \n\n[Species]\n%s" % (order, fam, sp))
      f.close()
    
m353_select(df)
quit()
```

now we concatenate the unpaired reads 

first we concatenate the unpaired reads in each folder
```{sh}
for file in *_1_1U*
do 
file2=${file//1[P|U]/2U}
file3=${file//1U/U}
cat $file $file2 > $file3
rm $file $file2
done

gunzip ./*/*.gz # unzip all of the reads

for i in `find . -name ".fastq"`; mmv -r '*fastq.*' '#1#2' ; done

rename _1_ _ ./*/*fastq
rename P _paired ./*/*fastq
rename U unpaired ./*/*fastq
rename _1_ _R1_ ./*/*fastq
rename _2_ _R2_ ./*/*fastq

find -name \*.fastq -printf "%f\n" | sed -e 's/_[P|U]_R[1|2]\.fastq$/  /' | sort -u > namelist.txt
```

than create this script in the root of the folder with all of our sequences by order
```{sh}
for file in *R1_paired*
do
file2=${file//R1/R2}
file3=${file//R1_paired.fastq/unpaired.fastq}
file4=${file//R1_paired.fastq/}
echo python /home/reed/HybPiper/reads_first.py -r $file $file2 --unpaired $file3 -b filtered_target_file.fasta --prefix $file4 --bwa >> reads_first.sh
done

# and copy it to each folder as so:

for d in */; do cp reads_first.sh "$d"; done
rm reads_first.sh # to remove the script from the root

# and then run it from each folder

bash reads_first.sh
```

Now update the SRA table code with the taxon's name.

```{r}
# look up and join the NCBI identifier after the kraken taxid code. 

$ R
library(taxize)

data <- rbind(read.csv('/home/reed/sequenceData/sra_requests.csv', col.names = 'SRA'), read.csv('/home/reed/sequenceData/sra_requests2.csv', col.names = 'SRA'))

sra_run_table <- read.csv('/home/reed/sequenceData/SraRunTable.txt')[,c('Run', 'Organism')]
sra_rt1 <- read.csv('/home/reed/sequenceData/SraRunTable(1).txt')[,c('Run', 'Organism')]
sra_rt2 <- read.csv('/home/reed/sequenceData/SraRunTable(2).txt')[,c('Run', 'Organism')]
sra_run_table <- rbind(sra_run_table, sra_rt1, sra_rt2)

data <- merge(data, sra_run_table, by.x = 'SRA', by.y = 'Run')

species <- unique(data$Organism)
pc <- get_uid(sci_com = species, key = '1dddef4ac326bf8b74fa69ae430411218f09')
ncbi_keys <- cbind(species, pc)

taxid_results <- merge(data, ncbi_keys, by.x = "Organism", by.y =  "species",  all.x = TRUE)
rm(species, pc, ncbi_keys, data)

directory <- '/media/reed/Genomics/kew_tol/to_pipe'
list.dirs.depth.n <- function(p, n) { # @ RolandASC
  res <- list.dirs(p, recursive = FALSE)
  if (n > 1) {
    add <- list.dirs.depth.n(res, n-1)
    c(res, add)
  } else {
    res
  }
}

folder_name <- list.dirs.depth.n(p = directory, n = 2)
folder_name <- sub('^.*/', '', folder_name)
folder_name <- grep("_", folder_name, value = TRUE)
folder_name <- tolower(folder_name)
sample_folder_names <- folder_name
sample_folders <- sub('_s[0-9]{2}_', '', folder_name)
sample_folders_squished <- gsub('-|_', '', sample_folders)

targets <- data.frame(folder_path = paste0(directory, '/', folder_name), 
                      sample_folders,
                      sample_folders_squished, 
                      sample_folder_names)

rm(sample_folders_squished, folder_name, sample_folder_names, sample_folders, directory, list.dirs.depth.n)

taxid_results$SRA <- tolower(taxid_results$SRA)
final_results <- merge(taxid_results, targets, 
                       by.x = 'SRA', by.y = 'sample_folders_squished',
                       all.x = T)
final_results <- final_results[c('Organism', 'pc', 'sample_folder_names')]

final_results_text <- final_results[,c('sample_folder_names', 'pc')]
final_results <- final_results[!duplicated(final_results),]
final_results_text$sample_folder_names <- toupper(final_results_text$sample_folder_names)
readr::write_delim(final_results_text, file = "taxid_lookup.txt")

rm(taxid_results, targets, final_results)
quit()
```

# now we use that form to update the lines in each file which contain the SRA run information with the taxid

```{sh}
$ cd ~/sequenceData/to_pipe

awk '
NR==FNR {
    old[NR] = $1
    gsub(/&/,RS,$2)
    new[NR] = $2
    next
}
{
    for (i=1; i in old; i++) {
        gsub(old[i],new[i])
    }
    gsub(RS,"\\&")
    print
}
' taxid_lookup.txt ./*/*_con.fasta >> rmbl_consensus.fasta

```


Finally we can build the database now!
`sh kraken2-build --build --db Eastwood --threads 16`


## Run metagenome samples through Kraken2

```{sh}
# first we will run trimmomatic on these files

find -type f -name "[0-9][0-9]_S[0-9]*_paired.fastq.gz" -exec cp {} ../raw_reads \;
find -type f -name "[0-9]_S[0-9]*_paired.fastq.gz" -exec cp {} ../raw_reads \;


# now run trimomatic
for R1 in *R1_001*; do
    R2=${R1//R1_001.fastq.gz/R2_001.fastq.gz}
    R1paired=${R1//.fastq/_paired.fastq}
    R1unpaired=${R1//.fastq/_unpaired.fastq}
    R2paired=${R2//.fastq/_paired.fastq}
    R2unpaired=${R2//.fastq/_unpaired.fastq}
    echo java -jar /home/reed/Trimmomatic-0.39/trimmomatic-0.39.jar PE -phred33 $R1 $R2 $R1paired $R1unpaired $R2paired $R2unpaired ILLUMINACLIP:/home/reed/Trimmomatic-0.39/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:5:20 MINLEN:30 >> trimmomatic.sh
done

$ bash trimmmomatic.sh

# this script mucks up the naming of the files, we will copy over the valid files like so. We will only use the Paired forward and reverse reads with trimmomatic

find -type f -name "[0-9]_S[0-9]*001_paired.fastq.gz" -exec cp {} ../trimmed_metagenomic \;

# be sure to repeat with the double [0-9] to get samples with two numbers.

#reduce the file name

mmv -r '*_001_*' '#1_#2'
mmv -r '*_L002_*' '#1_#2'
```

Great now we are ready to unlease the kraken!

```{sh}
for reed1 in *R1_paired*
do
reed2=${reed1//R1_paired.fastq.gz/R2_paired.fastq.gz}
samp_nom=$(basename "$reed1" _R1_paired.fastq.gz)
kraken2 --db ~/Eastwood --report ~/sequenceData/metagenome_class/${samp_nom_report.txt  --report-minimizer-data --threads 16 --gzip-compressed --paired $reed1 $reed2 --output  ~/sequenceData/metagenome_class/$samp_nom.txt
done
```


## run bracken on samples to try and reduce reads at the level of genus

download the zip file of bracken from https://github.com/jenniferlu717/Bracken andextract to your home directory

```{sh}
cd Bracken-master
bash install_bracken.sh
bash bracken-build -d ../Eastwood -t 16 -k 35 -l 150

# now run bracken

cd /home/reed/sequenceData/metagenome_class

for classification in *report.txt
do
    SAMPLE=${classification//.txt}
    FNAME=${SAMPLE//_report}
     ~/Bracken-master/bracken -d ~/Eastwood -i ${SAMPLE}.txt -o ${FNAME}.bracken -r 150 -t 100
done
```


## Create a local instance of BLAST

```{sh}
# prep the sequences
cat /media/reed/Genomics/kew_tol/to_pipe/tol_consensus.fasta ~/sequenceData/to_pipe/rmbl_consensus.fasta >> /media/reed/Genomics/rmbl_seqs/all_rmbl_seqs.fasta
cat all_rmbl_seqs.fasta | seqkit rmdup -n -o clean.fasta

sed -i -e 's/|kraken:taxid|/-/' all_rmbl_seqs.fasta # remove this section
sed -i -e 's/NODE/n/' all_rmbl_seqs.fasta
sed -i -e 's/length/l/' all_rmbl_seqs.fasta
# need to make the IDs shorter

grep '^>' all_rmbl_seqs.fasta > NCBEastwood_tax_map.txt
sed -i -e 's/-/ /2' NCBEastwood_tax_map.txt
sed -i -e 's/>//' NCBEastwood_tax_map.txt
sed -i -e 's/7B_S89_/53216/' NCBEastwood_tax_map.txt # just for us L. bakeri confusion

makeblastdb -in all_rmbl_seqs.fasta -parse_seqids -taxid_map NCBEastwood_tax_map.txt -title "NCBEastwood"  -dbtype nucl

# some node may actually be replicated... we dealt with this as so:

grep -R -i "NODE_551_LENGTH_79_COV_4.000000-5968" . # find it
sed -i -e 's/NODE_551_length_79_cov_4.000000-5968 2841764/NODE_551_length_79_cov_4.000000-5968 2841764 NODE_551_length_79_cov_4.000000-5968-2841764 2841764/' NCBEastwood_tax_map.txt # update to make one distinct.

sed -i -e 's/n_1_l_755_cov_9.377581-4471-1433329/n_1_l_755_cov_9.377581-4471-1433329a/' NCBEastwood_tax_map.txt # update to make one distinct.

```


You can then test to ensure that the BLAST database is working, here I test it on one sample.

```{sh}
blastn -query ~/sequenceData/trimmed_metagenomic/informative_seqs/3_S117.fasta -db /media/reed/Genomics/rmbl_seqs/all_rmbl_seqs.fasta -out ~/sequenceData/trimmed_metagenomic/informative_seqs/blast_results/3_S117_blast_res.txt

sed -n '/^Query=/,/^>/ { //!p }' test.txt > tables.txt

# we can run this process on many files as so:
cd ~/sequenceData/trimmed_metagenomic/informative_seqs/

# run blast on all sequences that were used to assign ID's
for f in *.fasta; do
    outfile="blast_results/${f/.fasta/}_seqs_blasted.txt"
    blastn -query $f -db /media/reed/Genomics/rmbl_seqs/all_rmbl_seqs.fasta -out $outfile
    echo "Sample:" ${f/.fasta/} "has been completed.";
done

# reformat output data for import to R
mkdir tables
mkdir raw_assign
for f in *blasted.txt; do
    outfile="tables/${f/_seqs_blasted.txt/}.txt"
    sed -n '/^Query=/,/^>/ { //!p }' $f | sed '/^Lambda/,/^Query/d' > $outfile
    mv $f "raw_assign/${f}" 
done


# Sequences producing significant alignments\
cd ~/sequenceData/trimmed_metagenomic/informative_seqs/blast_results/tables

for f in *.txt; do
    outfile="${f/.txt/}-cleaned.txt"
    cat $f | sed 's/Sequences producing significant alignments/alignment/' | sed 's/\*\*\*\*\* No hits found \*\*\*\*\*/no_results/' | sed 's/(Bits)//' | sed 's/Score//' | sed 's/E//' | sed 's/Value//' | awk '{ gsub(/[ ]+/," "); print }' > $outfile
    rm $f
done

```

Now we will pull in the BLAST results to see which loci generated most of the Kraken matches. 
